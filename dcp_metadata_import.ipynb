{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import requests\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from numpy import nan\n",
    "\n",
    "from tier1_to_dcp_dict import tier1_to_dcp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_id = 'bcb61471-2a44-4d00-a0af-ff085512674c'\n",
    "dataset_id = '0b75c598-0893-4216-afe8-5414cab7739d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "study_metadata = pd.read_csv(f\"metadata/{collection_id}_{dataset_id}_study_metadata.csv\", header=None).T\n",
    "study_metadata.columns = study_metadata.iloc[0]\n",
    "study_metadata.drop(0, axis=0, inplace=True)\n",
    "sample_metadata = pd.read_csv(f\"metadata/{collection_id}_{dataset_id}_metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_metadata = pd.read_csv('example/ImmuneLandscapeccRCC_metadata_30-01-2023_tier1_obs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hca_template_url = 'https://github.com/ebi-ait/geo_to_hca/raw/master/template/hca_template.xlsx'\n",
    "dcp_spreadsheet = pd.read_excel(hca_template_url, sheet_name=None, skiprows= [0,1,2,4])\n",
    "\n",
    "# save the 4-row header of the original spreadsheet with programmatic name as column names\n",
    "dcp_headers = pd.read_excel(hca_template_url, sheet_name=None, header=None)\n",
    "for tab in dcp_headers:\n",
    "    dcp_headers[tab].rename(columns=dcp_headers[tab].iloc[3], inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional additions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'doi' in sample_metadata and len(set(sample_metadata['doi'])) == 1:\n",
    "    dcp_spreadsheet['Project - Publications'] = pd.DataFrame({key: \\\n",
    "        (study_metadata['doi'].tolist() if key.endswith(\"doi\") \\\n",
    "            else [nan]) \\\n",
    "            for key in dcp_spreadsheet['Project - Publications'].keys()})\n",
    "\n",
    "if 'institute' in sample_metadata:\n",
    "    # TODO add institute per sample\n",
    "    if len(set(sample_metadata['institute'])) == 1:\n",
    "        dcp_spreadsheet['Cell suspension']['process.process_core.location'] = sample_metadata['institute'][0]\n",
    "if 'title' in sample_metadata:\n",
    "    if len(set(sample_metadata['title'])) != 1:\n",
    "        print(f\"We have multiple titles {set(sample_metadata['title'])}\")\n",
    "    dcp_spreadsheet['Project'] = pd.DataFrame({key: \\\n",
    "    (sample_metadata['title'][0] if key.endswith(\"project_title\") \\\n",
    "        else [nan]) \\\n",
    "        for key in dcp_spreadsheet['Project'].keys()})\n",
    "if 'study_pi' in  sample_metadata and \\\n",
    "    'institute' in sample_metadata:\n",
    "    #Â TODO add fix for multiple institutes per sample\n",
    "    if len(set(sample_metadata['study_pi'])) == 1 and \\\n",
    "        len(set(sample_metadata['institute'])) == 1:\n",
    "        study_pi_dict = {\n",
    "            'project.contributors.name': sample_metadata['study_pi'][0], \n",
    "            'project.contributors.institution': sample_metadata['institute'][0],\n",
    "            'project.contributors.corresponding_contributor': 'yes'\n",
    "            }\n",
    "        study_pi_dict.update({\n",
    "            key: nan for key in dcp_spreadsheet['Project - Contributors'].keys() if key not in study_pi_dict.keys()\n",
    "        })\n",
    "        dcp_spreadsheet['Project - Contributors'] = pd.DataFrame(study_pi_dict, index=[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_collection_relative_time_point' in sample_metadata:\n",
    "    number_pattern = '([\\\\d]+[.|\\\\,]?\\\\d?)'\n",
    "    sample_metadata['specimen_from_organism.biomaterial_core.timecourse.value'] = \\\n",
    "        sample_metadata['sample_collection_relative_time_point'].str.extract(number_pattern, expand=False)\n",
    "    sample_metadata.loc[sample_metadata['sample_collection_relative_time_point'].notna(), 'specimen_from_organism.biomaterial_core.timecourse.relevance'] = 'relative time of collection'\n",
    "    time_units_pattern = r'(hour|day|week|month|year)'\n",
    "    sample_metadata['specimen_from_organism.biomaterial_core.timecourse.unit.text'] = \\\n",
    "        sample_metadata['sample_collection_relative_time_point'].str.extract(time_units_pattern, expand=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'organism_ontology_term_id' in sample_metadata:\n",
    "    sample_metadata['donor_organism.biomaterial_core.ncbi_taxon_id'] = sample_metadata['organism_ontology_term_id'].str.removeprefix('NCBITaxon:')\n",
    "    sample_metadata['specimen_from_organism.biomaterial_core.ncbi_taxon_id'] = sample_metadata['organism_ontology_term_id'].str.removeprefix('NCBITaxon:')\n",
    "    if 'tissue_type' in sample_metadata and 'tissue' in sample_metadata['tissue_type']:\n",
    "        sample_metadata.loc[sample_metadata['tissue_type'] == 'tissue', 'cell_suspension.biomaterial_core.ncbi_taxon_id'] = \\\n",
    "            sample_metadata.loc[sample_metadata['tissue_type'] == 'tissue', 'organism_ontology_term_id'].str.removeprefix('NCBITaxon:')\n",
    "    if 'tissue_type' in sample_metadata and 'cell culture' in sample_metadata['tissue_type']:\n",
    "        sample_metadata.loc[sample_metadata['tissue_type'] == 'cell culture', 'cell_line.biomaterial_core.ncbi_taxon_id'] = \\\n",
    "            sample_metadata.loc[sample_metadata['tissue_type'] == 'cell culture', 'organism_ontology_term_id'].str.removeprefix('NCBITaxon:')\n",
    "    if 'tissue_type' in sample_metadata and 'organoid' in sample_metadata['tissue_type']:\n",
    "        sample_metadata.loc[sample_metadata['tissue_type'] == 'organoid', 'organoid.biomaterial_core.ncbi_taxon_id'] = \\\n",
    "            sample_metadata.loc[sample_metadata['tissue_type'] == 'organoid', 'organism_ontology_term_id'].str.removeprefix('NCBITaxon:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sex_ontology_dict = {\n",
    "        'PATO:0000383': 'female',\n",
    "        'PATO:0000384': 'male'\n",
    "}           \n",
    "if 'sex_ontology_term_id' in sample_metadata:\n",
    "    sample_metadata['donor_organism.sex'] = sample_metadata['sex_ontology_term_id'].replace(sex_ontology_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sample_source' in sample_metadata:\n",
    "    sample_metadata['specimen_from_organism.transplant_organ'] = sample_metadata.apply(lambda x: 'yes' if x['sample_source'] == 'organ_donor' else 'no', axis=1)\n",
    "    if any((sample_metadata['sample_source'] == 'postmortem donor') & (sample_metadata['manner_of_death'] == 'not applicable')) or \\\n",
    "       any((sample_metadata['sample_source'] != 'postmortem donor') & (sample_metadata['manner_of_death'] != 'not applicable')):\n",
    "        print(f'Conflicting metadata {sample_metadata.loc[(sample_metadata['sample_source'] == 'postmortem donor') & (sample_metadata['manner_of_death'] == 'not applicable'), ['sample_source', 'manner_of_death']]}')\n",
    "        print(f'Conflicting metadata {sample_metadata.loc[(sample_metadata['sample_source'] != 'postmortem donor') & (sample_metadata['manner_of_death'] != 'not applicable'), ['sample_source', 'manner_of_death']]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardy_scale = [0, 1, 2, 3, 4, '0', '1', '2', '3', '4']\n",
    "manner_of_death_is_living_dict = {n: 'no' for n in hardy_scale}\n",
    "manner_of_death_is_living_dict.update({'unknown': 'no', 'not applicable': 'yes'})\n",
    "\n",
    "if 'manner_of_death' in sample_metadata:\n",
    "    sample_metadata['donor_organism.is_living'] = sample_metadata['manner_of_death'].replace(manner_of_death_is_living_dict)\n",
    "    sample_metadata['donor_organism.death.hardy_scale'] = sample_metadata.apply(lambda x: x['manner_of_death'] if x['manner_of_death'] in hardy_scale else nan, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sampled_site_condition' in sample_metadata:\n",
    "    # if sampled_site_condition is adjacent, we fill adjacent_diseases with disease_ontology_term_id\n",
    "    # if diseased: known_diseases = disease_ontology_term_id and adjacent = nan\n",
    "    # if healthy: known_diseases = disease_ontology_term_id or PATO:0000461 and adjacent = nan\n",
    "    def sampled_site_to_known_diseases(row):\n",
    "        if row['sampled_site_condition'] == 'adjacent' and 'disease_ontology_term_id' in row:\n",
    "            return ['PATO:0000461', row['disease_ontology_term_id']]\n",
    "            if row['disease_ontology_term_id'] != 'PATO:0000461':\n",
    "                print(f'Conflicting metadata {row[['sampled_site_condition', 'disease_ontology_term_id']]}')\n",
    "        elif row['sampled_site_condition'] in ['healthy', 'diseased'] and 'disease_ontology_term_id' in row:\n",
    "            return [row['disease_ontology_term_id'], nan]\n",
    "        elif row['sampled_site_condition'] == 'healthy':\n",
    "            return ['PATO:0000461', nan]\n",
    "        else:\n",
    "            return [nan, nan]\n",
    "\n",
    "    sample_metadata[['specimen_from_organism.diseases.ontology', 'specimen_from_organism.adjacent_diseases.ontology']] = \\\n",
    "        sample_metadata.apply(sampled_site_to_known_diseases, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we expect this to become an enum, when it would be easier to extract\n",
    "# for now we extract the numbers as in the example pattern\n",
    "if 'alignment_software' in sample_metadata:\n",
    "    sample_metadata[['analysis_protocol.alignment_software', 'analysis_protocol.alignment_software_version']] = \\\n",
    "        sample_metadata['alignment_software'].str.extract(r'([\\w\\s]+)\\s(v?[\\d\\.]+)')\n",
    "    no_version = ~sample_metadata['alignment_software'].str.match(r'.*v?[\\d\\.]+')\n",
    "    sample_metadata.loc[no_version, 'analysis_protocol.alignment_software'] = \\\n",
    "            sample_metadata.loc[no_version, 'alignment_software']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'library_sequencing_run' in sample_metadata:\n",
    "    insdc_run_pattern = r'^[D|E|S]RR[0-9]+(\\|\\|[D|E|S]RR[0-9]+)*$'\n",
    "    separator = sample_metadata['library_sequencing_run'].str.extract(r'([^D|E|S|R|0-9]+)', expand=False).dropna().unique()\n",
    "    if len(separator) > 0:\n",
    "        sample_metadata['sequence_file.insdc_run_accessions'] = sample_metadata['library_sequencing_run'].str.replace(separator[0], '||')\n",
    "    else:\n",
    "        sample_metadata['sequence_file.insdc_run_accessions'] = sample_metadata['library_sequencing_run']\n",
    "    if not sample_metadata['sequence_file.insdc_run_accessions'].str.match(insdc_run_pattern).all():\n",
    "        print('Following library sequencing run IDs does not match the INSDC pattern')\n",
    "        display(sample_metadata.loc[~sample_metadata['sequence_file.insdc_run_accessions'].str.match(insdc_run_pattern), ['sample_id', 'library_id', 'sequence_file.insdc_run_accessions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the CL name instead of ontology for markers enrichment\n",
    "\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "def hcao_label(ontology, regex=r'\\w+:\\d+'):\n",
    "    if not re.match(regex, ontology):\n",
    "        return ontology\n",
    "    response = requests.get(f'https://ontology.archive.data.humancellatlas.org/api/terms?id={ontology}')\n",
    "    try:\n",
    "        results = response.json()['_embedded']['terms']\n",
    "    except ConnectionError as e:\n",
    "        print(e)\n",
    "    return results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cl_label(ontology):\n",
    "    result = hcao_label(ontology)\n",
    "    if isinstance(result, dict):\n",
    "        return result['label']\n",
    "    elif isinstance(result, str):\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'cell_enrichment' in sample_metadata:\n",
    "    sample_metadata['enrichment_protocol.markers'] = sample_metadata['cell_enrichment'].apply(cl_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dev_label(ontology):\n",
    "    start_id = ['start, days post fertilization', 'start, months post birth', 'start, years post birth']\n",
    "    end_id = ['end, days post fertilization', 'end, months post birth', 'end, years post birth']\n",
    "    print(f'Starting ontology {ontology}')\n",
    "    result = hcao_label(ontology)\n",
    "    if isinstance(result, dict):\n",
    "        if not 'annotation' in result:\n",
    "            print(f'Ontology {ontology} does not have annotation')\n",
    "            return ontology\n",
    "        start_key = [key for key in result['annotation'].keys() if key in start_id]\n",
    "        if len(start_key) == 0:\n",
    "            print(f'Ontology {ontology} does not have start annotation')\n",
    "            return ontology\n",
    "        elif len(start_key) > 1:\n",
    "            print(f'Multiple start IDs {start_key}. Selecting the smallest value {start_key[0]}')\n",
    "        unit_time = start_key[0].split(\" \")[1].rstrip('s')\n",
    "        range = [str(int(float(result['annotation'][start_key[0]][0])))]\n",
    "        end_key = [key for key in result['annotation'].keys() if key in end_id]\n",
    "        if len(end_key) == 0:\n",
    "            return ' '.join([range[0],unit_time])\n",
    "        result['annotation'][end_key[0]] = [str(int(float(result['annotation'][end_key[0]][0])) - 1)]\n",
    "        range.extend(result['annotation'][end_key[0]])\n",
    "        if float(range[1]) - float(range[0]) == 1:\n",
    "            return ' '.join([range[0],unit_time])\n",
    "        return ' '.join(['-'.join(range),unit_time])\n",
    "    elif isinstance(result, str):\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local map of the most used ranges to reduce \n",
    "dev_to_age_dict = {\n",
    "    'HsapDv:0000264': '0-14 year',\n",
    "    'HsapDv:0000268': '15-19 year',\n",
    "    'HsapDv:0000237': '20-29 year',\n",
    "    'HsapDv:0000238': '30-39 year',\n",
    "    'HsapDv:0000239': '40-49 year',\n",
    "    'HsapDv:0000240': '50-59 year',\n",
    "    'HsapDv:0000241': '60-69 year',\n",
    "    'HsapDv:0000242': '70-79 year',\n",
    "    'HsapDv:0000243': '80-89 year'\n",
    "}\n",
    "\n",
    "if 'development_stage_ontology_term_id' in sample_metadata:\n",
    "    sample_metadata[['donor_organism.organism_age', 'donor_organism.organism_age_unit.text']] = \\\n",
    "        sample_metadata['development_stage_ontology_term_id']\\\n",
    "            .apply(lambda x: dev_to_age_dict[x] if x in dev_to_age_dict.keys() else dev_label(x))\\\n",
    "            .str.split(' ', expand=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename df & generate spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcp_flat = sample_metadata.rename(columns=tier1_to_dcp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tab in dcp_spreadsheet:\n",
    "    keys_union = [key for key in dcp_spreadsheet[tab].keys() if key in dcp_flat.keys()]\n",
    "    # if tab contains only the input biomaterial name, then skip the tab\n",
    "    if (len(keys_union) == 1) and (tab.lower().replace(\" \", \"_\") != keys_union[0].split(\".\")[0]):\n",
    "        continue\n",
    "    # collapse arrays in duplicated columns\n",
    "    if any(dcp_flat[keys_union].columns.duplicated()):\n",
    "        for dub_cols in set(dcp_flat[keys_union].columns[dcp_flat[keys_union].columns.duplicated()]):\n",
    "            df = dcp_flat[dub_cols]\n",
    "            dcp_flat.drop(columns=dub_cols, inplace=True)\n",
    "            dcp_flat[dub_cols] = df[dub_cols].apply(lambda x: '||'.join(x.dropna().astype(str)),axis=1)\n",
    "\n",
    "    # merge the two dataframes\n",
    "    dcp_spreadsheet[tab] = pd.concat([dcp_spreadsheet[tab],dcp_flat[keys_union]])\n",
    "    dcp_spreadsheet[tab] = dcp_spreadsheet[tab].dropna(how='all').drop_duplicates()\n",
    "\n",
    "    # generate a unique protocol_id\n",
    "    if tab.endswith('protocol') and keys_union:\n",
    "        dcp_spreadsheet[tab] = dcp_spreadsheet[tab].drop_duplicates()\n",
    "        # there should be only 1 protocol_id in each protocol tab. we need a series to replace spaces\n",
    "        protocol_id_col = [col for col in dcp_spreadsheet[tab].columns if col.endswith('protocol_core.protocol_id')][0]\n",
    "        dcp_spreadsheet[tab][protocol_id_col] = [tab.lower().replace(\" \",\"_\") + \"_\" + str(n + 1) for n in range(len(dcp_spreadsheet[tab]))]\n",
    "\n",
    "    if tab == 'Project':\n",
    "        dcp_spreadsheet[tab] = dcp_spreadsheet[tab].drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We should have 1 only Analysis file with all the CS merged\n",
    "\n",
    "def collapse_values(series):\n",
    "    return \"||\".join(series.unique().astype(str))\n",
    "\n",
    "dcp_spreadsheet['Analysis file']['analysis_file.file_core.file_name'] = f'{collection_id}_{dataset_id}.h5ad'\n",
    "dcp_spreadsheet['Analysis file']['analysis_file.file_core.content_description.text'] = 'count matrix'\n",
    "dcp_spreadsheet['Analysis file']['analysis_file.file_core.file_source'] = 'CxG'\n",
    "dcp_spreadsheet['Analysis file']['analysis_file.file_core.format'] = 'h5ad'\n",
    "\n",
    "\n",
    "adata_protocol_ids = {\n",
    "    'Library preparation protocol': 'library_preparation_protocol.protocol_core.protocol_id',\n",
    "    'Sequencing protocol': 'sequencing_protocol.protocol_core.protocol_id',\n",
    "    'Analysis protocol': 'analysis_protocol.protocol_core.protocol_id'\n",
    "}\n",
    "\n",
    "for tab, id in adata_protocol_ids.items():\n",
    "    dcp_spreadsheet['Analysis file'][id] = \\\n",
    "    collapse_values(\\\n",
    "        dcp_spreadsheet[tab][id]\n",
    "    )\n",
    "\n",
    "\n",
    "dcp_spreadsheet['Analysis file'] = dcp_spreadsheet['Analysis file']\\\n",
    "    .groupby('analysis_file.file_core.file_name')\\\n",
    "    .agg(collapse_values)\\\n",
    "    .reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project\n",
      "Project - Contributors\n",
      "Donor organism\n",
      "Specimen from organism\n",
      "Cell suspension\n",
      "Sequence file\n",
      "Collection protocol\n",
      "Enrichment protocol\n",
      "Library preparation protocol\n",
      "Sequencing protocol\n",
      "Analysis file\n",
      "Analysis protocol\n"
     ]
    }
   ],
   "source": [
    "with pd.ExcelWriter(f\"metadata/{collection_id}_{dataset_id}_dcp.xlsx\") as writer:\n",
    "    for tab in dcp_spreadsheet:\n",
    "        if not dcp_spreadsheet[tab].empty:\n",
    "            print(tab)\n",
    "            pd.concat([dcp_headers[tab], dcp_spreadsheet[tab]]).to_excel(writer, index=False, sheet_name=tab, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pandas",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
